import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
import json
import pandas as pd

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ Pydantic-—Å—Ö–µ–º—ã –∏–∑ —Ñ–∞–π–ª–∞ schema.py
from schema import ResumeData, JobMatchResult

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–≤–∫–ª—é—á–∞—è OPENAI_API_KEY)
load_dotenv()

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏ ---
# –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã —Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏)
if 'results' not in st.session_state:
    st.session_state.results = None


# --- –õ–û–ì–ò–ö–ê LANGCHAIN ---

@st.cache_data
def process_resume(file_content, file_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Ä–µ–∑—é–º–µ."""
    temp_dir = "temp"
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    
    file_path = os.path.join(temp_dir, file_name)
    with open(file_path, "wb") as f:
        f.write(file_content)

    try:
        if file_name.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif file_name.endswith(".docx"):
            loader = Docx2txtLoader(file_path)
        else:
            return None, "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞."

        documents = loader.load()
        resume_text = " ".join([doc.page_content for doc in documents])
        
        parser_resume = PydanticOutputParser(pydantic_object=ResumeData)
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0)
        
        prompt_template_resume = """
        –í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–µ–∫—Ä—É—Ç–µ—Ä. –ò–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ.
        –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ: --- {resume_text} ---
        –°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é. {format_instructions}
        """
        prompt_resume = ChatPromptTemplate.from_template(
            template=prompt_template_resume,
            partial_variables={"format_instructions": parser_resume.get_format_instructions()}
        )
        extraction_chain = prompt_resume | llm | parser_resume
        extracted_data = extraction_chain.invoke({"resume_text": resume_text})
        return extracted_data, None
        
    except Exception as e:
        return None, f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ {file_name}: {e}"
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

def run_job_match_analysis(_resume_data_dict, job_description):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–¥–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–∏."""
    try:
        resume_data_str = json.dumps(_resume_data_dict, ensure_ascii=False, indent=2)
        parser_match = PydanticOutputParser(pydantic_object=JobMatchResult)
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

        prompt_template_match = """
        –¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π HR-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤–∞–∫–∞–Ω—Å–∏–∏.
        –í–æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
        ---
        {resume_data}
        ---
        –ê –≤–æ—Ç —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:
        ---
        {job_description}
        ---
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞:
        1. –û—Ü–µ–Ω–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ —à–∫–∞–ª–µ –æ—Ç 1 –¥–æ 10.
        2. –ù–∞–ø–∏—Å–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (3-4 –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–∞) –æ —Ç–æ–º, –ø–æ—á–µ–º—É –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–æ–¥—Ö–æ–¥–∏—Ç –∏–ª–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç.
        3. –£–∫–∞–∑–∞—Ç—å, –∫–∞–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç—É.
        
        –°–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é. {format_instructions}
        """
        prompt_match = ChatPromptTemplate.from_template(
            template=prompt_template_match,
            partial_variables={"format_instructions": parser_match.get_format_instructions()}
        )
        match_chain = prompt_match | llm | parser_match
        match_result = match_chain.invoke({"resume_data": resume_data_str, "job_description": job_description})
        return match_result, None

    except Exception as e:
        return None, f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {e}"

# --- –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.set_page_config(page_title="AI Resume Analyzer (Batch Mode)", layout="wide")
st.title("üöÄ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ")

# --- –ë–ª–æ–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ---
with st.container(border=True):
    st.header("1. –í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    job_description_text = st.text_area("–¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:", height=150, key="job_description")
    uploaded_files = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤)", 
        type=["pdf", "docx"],
        accept_multiple_files=True
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("–ù–∞—á–∞—Ç—å –ø–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑", type="primary", use_container_width=True):
            if not uploaded_files:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª —Å —Ä–µ–∑—é–º–µ.")
            elif not job_description_text:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏.")
            else:
                all_results = []
                progress_bar = st.progress(0, text="–ê–Ω–∞–ª–∏–∑ –Ω–∞—á–∞—Ç...")

                for i, uploaded_file in enumerate(uploaded_files):
                    progress_text = f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {uploaded_file.name} ({i+1}/{len(uploaded_files)})..."
                    progress_bar.progress((i + 1) / len(uploaded_files), text=progress_text)
                    
                    file_bytes = uploaded_file.getvalue()
                    resume_data, error = process_resume(file_bytes, uploaded_file.name)
                    if error:
                        st.error(f"–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {uploaded_file.name}: {error}")
                        continue

                    match_result, match_error = run_job_match_analysis(resume_data.dict(), job_description_text)
                    if match_error:
                        st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ {resume_data.full_name}: {match_error}")
                        continue

                    all_results.append({
                        "file_name": uploaded_file.name,
                        "resume_data": resume_data,
                        "match_result": match_result
                    })
                
                progress_bar.empty()
                st.session_state.results = all_results # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–µ—Å—Å–∏—é
    with col2:
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", use_container_width=True):
            st.session_state.results = None
            st.rerun() # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, —á—Ç–æ–±—ã –æ—á–∏—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

# --- –ë–ª–æ–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
if st.session_state.results:
    st.success(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(st.session_state.results)} —Ä–µ–∑—é–º–µ.")
    
    # --- –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã ---
    summary_data = []
    for res in st.session_state.results:
        summary_data.append({
            "–ò–º—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞": res["resume_data"].full_name,
            "Email": res["resume_data"].email,
            "–û—Ü–µ–Ω–∫–∞": res["match_result"].score,
            "–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã": res["match_result"].summary,
            "–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –Ω–∞–≤—ã–∫–∏": ", ".join(res["match_result"].missing_skills) if res["match_result"].missing_skills else "–ù–µ—Ç"
        })
    df = pd.DataFrame(summary_data)

    # --- –í–∫–ª–∞–¥–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è ---
    tab_summary, tab_json = st.tabs(["–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞", "–î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (JSON)"])

    with tab_summary:
        st.header("–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        st.dataframe(df)

        @st.cache_data
        def convert_df_to_csv(_df):
            return _df.to_csv(index=False).encode('utf-8')

        csv = convert_df_to_csv(df)
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV",
            data=csv,
            file_name="resume_analysis_results.csv",
            mime="text/csv",
        )

    with tab_json:
        st.header("–î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–¥–∏–¥–∞—Ç—É")
        for res in st.session_state.results:
            with st.expander(f"üìÑ {res['resume_data'].full_name or res['file_name']}"):
                st.subheader("–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—é–º–µ")
                st.json(res["resume_data"].model_dump())
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")
                st.json(res["match_result"].model_dump())

